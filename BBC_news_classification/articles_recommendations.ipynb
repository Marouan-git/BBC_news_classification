{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "# import ast\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Books using Doc2Vec\n",
    "\n",
    "### Approach:\n",
    "1. **Train a Doc2Vec Model:**\n",
    "   - Utilize Doc2Vec to transform entire articles into continuous vector representations.\n",
    "   - Train the model to capture contextual information and semantic meaning of each document.\n",
    "\n",
    "2. **Compute Similarity:**\n",
    "   - Measure similarity between articles based on the cosine similarity of their Doc2Vec vectors.\n",
    "   - Cosine similarity provides a metric for how closely the semantic meanings align.\n",
    "\n",
    "3. **Benefits of Doc2Vec:**\n",
    "   - Doc2Vec models excel in capturing contextual nuances and semantic relationships within entire documents.\n",
    "\n",
    "This approach enables the recommendation of books based on their semantic similarity, allowing for a more nuanced understanding of content beyond simple keyword matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation of articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/BBC_News_Train_PREPROCESSED.csv\")\n",
    "documents = df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16820695, -0.16386934, -0.11466306,  0.17859745,  0.02471507,\n",
       "       -0.13082625, -0.02174445,  0.19763207, -0.40533647, -0.1359295 ,\n",
       "        0.03789003, -0.16246106,  0.03443849,  0.00705553,  0.05489259,\n",
       "       -0.05401694,  0.00959428,  0.20757201, -0.13236567, -0.43132684,\n",
       "       -0.05026904,  0.04216481,  0.05836882,  0.0348097 ,  0.07203481,\n",
       "        0.05889795, -0.24727908, -0.21111465,  0.10123955, -0.08069197,\n",
       "        0.2861252 ,  0.013252  , -0.02052416, -0.1765319 , -0.0586794 ,\n",
       "        0.19217052,  0.05463048, -0.23824358, -0.1140677 , -0.00863943,\n",
       "        0.08923928, -0.17095836, -0.00069714, -0.00612225,  0.1396039 ,\n",
       "       -0.2001712 , -0.07856872, -0.1913196 ,  0.17930265, -0.09241138,\n",
       "       -0.0340836 , -0.09832505, -0.2480383 , -0.01755597,  0.00226811,\n",
       "        0.23376927,  0.00194942,  0.01123526,  0.04868018,  0.04661107,\n",
       "       -0.03506247,  0.1122677 ,  0.25407228,  0.05757903, -0.21821968,\n",
       "        0.14918123, -0.06852049,  0.04628516, -0.1304316 ,  0.00744418,\n",
       "       -0.06735501,  0.1813321 ,  0.02142582, -0.10969512,  0.04288325,\n",
       "        0.07395343, -0.05471191, -0.12368026, -0.03643565,  0.13944556,\n",
       "        0.07014701, -0.11256505, -0.16319482,  0.03503298,  0.01537433,\n",
       "       -0.0605271 , -0.03320996,  0.03160982,  0.06090471,  0.01496157,\n",
       "        0.06252352,  0.07552975, -0.18627083, -0.05720144, -0.09379645,\n",
       "        0.04500469,  0.10358227, -0.03969701,  0.15344165,  0.02936137],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Training the Doc2Vec model\n",
    "vector_size = 100\n",
    "model = Doc2Vec(vector_size=vector_size, window=5, min_count=1, workers=4, epochs=10)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Retrieving vectors for existing documents\n",
    "existing_document_index = 0\n",
    "existing_document_vector = model.dv[existing_document_index]\n",
    "existing_document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vectoriser un nouveau document, il suffira d'Ã©xecuter les lignes suivantes : \n",
    "\n",
    "`new_document = ['new', 'document', 'to', 'vectorize']`<br>\n",
    "`vector = model.infer_vector(new_document)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(row):\n",
    "    return model.dv[row.name]\n",
    "\n",
    "# Apply the function to create the 'vectors' column\n",
    "df['vectors'] = df.apply(get_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../assets/model_Doc2Vec.joblib']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"../data/vectorized_articles.csv\")\n",
    "dump(model, \"../assets/model_Doc2Vec.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute most similiar article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(article):\n",
    "    # Tokenize the article\n",
    "    words = article.split()\n",
    "\n",
    "    # Remove punctuation\n",
    "    punctuation = string.punctuation\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores(article, articles, model):\n",
    "    article = preprocess_article(article)\n",
    "    article_vector = model.infer_vector(article)\n",
    "\n",
    "    vectors = articles['vectors']\n",
    "    i = 0\n",
    "    similarity_score_dict = dict()\n",
    "    for vector in vectors:\n",
    "\n",
    "        # Reshape the vectors to be 2D arrays with a single row\n",
    "        vector = vector.reshape(1, -1)\n",
    "        article_vector = article_vector.reshape(1, -1)\n",
    "\n",
    "        similarity_score = cosine_similarity(vector, article_vector)[0, 0]\n",
    "\n",
    "        similarity_score_dict[i] = similarity_score\n",
    "        i += 1\n",
    "\n",
    "    return similarity_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_articles(article, category):\n",
    "    articles = pd.read_csv(\"../data/vectorized_articles.csv\")\n",
    "    articles = articles[articles[\"Category\"] == category]\n",
    "\n",
    "    model = load(\"../assets/model_Doc2Vec.joblib\")\n",
    "\n",
    "    scores = compute_similarity_scores(article,articles, model)\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    top_three_keys = list(sorted_scores.keys())[:3]\n",
    "\n",
    "    top_three_articles_df = df.loc[top_three_keys]\n",
    "\n",
    "    return top_three_articles_df[\"Text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv(\"../data/vectorized_articles.csv\")\n",
    "articles = articles[\"vectors\"]\n",
    "type(articles.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[-0.16623871 -0.12261564 -0.06123098  0.105811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1552</td>\n",
       "      <td>moving mobile improves golf swing mobile phone...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[ 0.01223124  0.02895926 -0.01085722  0.060944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>405</td>\n",
       "      <td>bt boost broadband package british telecom sai...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[-4.17283475e-02 -2.03106582e-01  5.38718328e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>702</td>\n",
       "      <td>peertopeer net stay peertopeer p2p network sta...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[-5.94673097e-01 -2.43753478e-01  1.41981453e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1951</td>\n",
       "      <td>pompeii get digital makeover oldfashioned audi...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[-0.28353983  0.08798035 -0.1239863   0.418030...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1  Unnamed: 0  ArticleId  \\\n",
       "3              3           3       1976   \n",
       "19            19          19       1552   \n",
       "24            24          24        405   \n",
       "26            26          26        702   \n",
       "30            30          30       1951   \n",
       "\n",
       "                                                 Text Category  \\\n",
       "3   lifestyle governs mobile choice faster better ...     tech   \n",
       "19  moving mobile improves golf swing mobile phone...     tech   \n",
       "24  bt boost broadband package british telecom sai...     tech   \n",
       "26  peertopeer net stay peertopeer p2p network sta...     tech   \n",
       "30  pompeii get digital makeover oldfashioned audi...     tech   \n",
       "\n",
       "                                              vectors  \n",
       "3   [-0.16623871 -0.12261564 -0.06123098  0.105811...  \n",
       "19  [ 0.01223124  0.02895926 -0.01085722  0.060944...  \n",
       "24  [-4.17283475e-02 -2.03106582e-01  5.38718328e-...  \n",
       "26  [-5.94673097e-01 -2.43753478e-01  1.41981453e-...  \n",
       "30  [-0.28353983  0.08798035 -0.1239863   0.418030...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv(\"../data/vectorized_articles.csv\")\n",
    "articles = articles[articles[\"Category\"] == \"tech\"]\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc = load(\"../assets/SVC.joblib\")\n",
    "articles_test = pd.read_csv(\"../data/BBC_News_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software watching while you work software that can not only monitor every keystroke and action performed at a pc but also be used as legally binding evidence of wrong-doing has been unveiled.  worries about cyber-crime and sabotage have prompted many employers to consider monitoring employees. the developers behind the system claim it is a break-through in the way data is monitored and stored. but privacy advocates are concerned by the invasive nature of such software.  the system is a joint venture between security firm 3ami and storage specialists bridgehead software. they have joined forces to create a system which can monitor computer activity  store it and retrieve disputed files within minutes. more and more firms are finding themselves in deep water as a result of data misuse. sabotage and data theft are most commonly committed from within an organisation according to the national hi-tech crime unit (nhtcu) a survey conducted on its behalf by nop found evidence that more than 80% of medium and large companies have been victims of some form of cyber-crime. bridgehead software has come up with techniques to prove  to a legal standard  that any stored file on a pc has not been tampered with. ironically the impetus for developing the system came as a result of the freedom of information act  which requires companies to store all data for a certain amount of time.  the storage system has been incorporated into an application developed by security firm 3ami which allows every action on a computer to be logged. potentially it could help employers to follow the trail of stolen files and pinpoint whether they had been emailed to a third party  copied  printed  deleted or saved to cd  floppy disk  memory stick or flash card. other activities the system can monitor include the downloading of pornography  the use of racist or bullying language or the copying of applications for personal use. increasingly organisations that handle sensitive data  such as governments  are using biometric log-ins such as fingerprinting to provide conclusive proof of who was using a particular machine at any given time. privacy advocates are concerned that monitoring at work is not only damaging to employee s privacy but also to the relationship between employers and their staff.  that is not the case   said tim ellsmore  managing director of 3ami.  it is not about replacing dialogue but there are issues that you can talk through but you still need proof   he said.  people need to recognise that you are using a pc as a representative of a company and that employers have a legal requirement to store data   he added.\n"
     ]
    }
   ],
   "source": [
    "article = articles_test.iloc[1][\"Text\"]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = svc.predict([article])[0]\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems coherent and correct. Let us continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmost_similar_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 7\u001b[0m, in \u001b[0;36mmost_similar_articles\u001b[1;34m(article, category)\u001b[0m\n\u001b[0;32m      3\u001b[0m articles \u001b[38;5;241m=\u001b[39m articles[articles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m category]\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../assets/model_Doc2Vec.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_similarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m sorted_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     10\u001b[0m top_three_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sorted_scores\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[1;32mIn[124], line 11\u001b[0m, in \u001b[0;36mcompute_similarity_scores\u001b[1;34m(article, articles, model)\u001b[0m\n\u001b[0;32m      7\u001b[0m similarity_score_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m vectors:\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Reshape the vectors to be 2D arrays with a single row\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[43mvector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     article_vector \u001b[38;5;241m=\u001b[39m article_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     similarity_score \u001b[38;5;241m=\u001b[39m cosine_similarity(vector, article_vector)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "most_similar_articles(article, category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBC_news_classification-cIhFtWaR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
