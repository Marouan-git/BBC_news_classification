{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "# import ast\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Books using Doc2Vec\n",
    "\n",
    "### Approach:\n",
    "1. **Train a Doc2Vec Model:**\n",
    "   - Utilize Doc2Vec to transform entire articles into continuous vector representations.\n",
    "   - Train the model to capture contextual information and semantic meaning of each document.\n",
    "\n",
    "2. **Compute Similarity:**\n",
    "   - Measure similarity between articles based on the cosine similarity of their Doc2Vec vectors.\n",
    "   - Cosine similarity provides a metric for how closely the semantic meanings align.\n",
    "\n",
    "3. **Benefits of Doc2Vec:**\n",
    "   - Doc2Vec models excel in capturing contextual nuances and semantic relationships within entire documents.\n",
    "\n",
    "This approach enables the recommendation of books based on their semantic similarity, allowing for a more nuanced understanding of content beyond simple keyword matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation of articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/BBC_News_Train_PREPROCESSED.csv\")\n",
    "documents = df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12892829, -0.14085585, -0.06172242,  0.15094805, -0.16697381,\n",
       "        0.11059604, -0.05945716, -0.02944281, -0.26169553, -0.06190001,\n",
       "       -0.0919568 , -0.09961358, -0.08542875,  0.0278306 , -0.04654634,\n",
       "       -0.15304457,  0.15102224,  0.13160914,  0.00130734, -0.23104867,\n",
       "        0.14889644,  0.05780197,  0.14653105,  0.08296006,  0.00786588,\n",
       "        0.05391062, -0.36035788, -0.03804403,  0.02786314, -0.04631222,\n",
       "        0.4705585 ,  0.06089744, -0.04739401, -0.14474416, -0.16000086,\n",
       "       -0.12576482, -0.16102254, -0.03453794, -0.15830232,  0.01161368,\n",
       "        0.06650274,  0.01028084,  0.18908545, -0.00852653,  0.04990214,\n",
       "       -0.16647975,  0.03139812, -0.10031002,  0.08946379, -0.09847921,\n",
       "       -0.04651788,  0.00546268, -0.06035695, -0.06309605, -0.05641305,\n",
       "        0.12740244, -0.02256392,  0.0020253 ,  0.02265512,  0.05173049,\n",
       "        0.04746727,  0.03716122,  0.29919875,  0.16928664,  0.00595477,\n",
       "        0.10677752,  0.07032375,  0.11482392, -0.04068539,  0.03147308,\n",
       "       -0.07694066,  0.17424831,  0.00802459, -0.16361476, -0.0118871 ,\n",
       "        0.1564143 ,  0.05716102, -0.04575812,  0.03047533, -0.01532395,\n",
       "       -0.06772293, -0.13137843, -0.11620968,  0.09834027,  0.04665079,\n",
       "       -0.08332928,  0.09796492, -0.10486431,  0.09119651, -0.02637664,\n",
       "        0.07367474,  0.14528666, -0.20880948, -0.10694977, -0.03375373,\n",
       "        0.07699401,  0.11871337,  0.04285951,  0.16123356,  0.15882167],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Training the Doc2Vec model\n",
    "vector_size = 100\n",
    "model = Doc2Vec(vector_size=vector_size, window=5, min_count=1, workers=4, epochs=10)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Retrieving vectors for existing documents\n",
    "existing_document_index = 0\n",
    "existing_document_vector = model.dv[existing_document_index]\n",
    "existing_document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vectoriser un nouveau document, il suffira d'Ã©xecuter les lignes suivantes : \n",
    "\n",
    "`new_document = ['new', 'document', 'to', 'vectorize']`<br>\n",
    "`vector = model.infer_vector(new_document)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(row):\n",
    "    return model.dv[row.name]\n",
    "\n",
    "# Apply the function to create the 'vectors' column\n",
    "df['vectors'] = df.apply(get_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12892829, -0.14085585, -0.06172242,  0.15094805, -0.16697381,\n",
       "        0.11059604, -0.05945716, -0.02944281, -0.26169553, -0.06190001,\n",
       "       -0.0919568 , -0.09961358, -0.08542875,  0.0278306 , -0.04654634,\n",
       "       -0.15304457,  0.15102224,  0.13160914,  0.00130734, -0.23104867,\n",
       "        0.14889644,  0.05780197,  0.14653105,  0.08296006,  0.00786588,\n",
       "        0.05391062, -0.36035788, -0.03804403,  0.02786314, -0.04631222,\n",
       "        0.4705585 ,  0.06089744, -0.04739401, -0.14474416, -0.16000086,\n",
       "       -0.12576482, -0.16102254, -0.03453794, -0.15830232,  0.01161368,\n",
       "        0.06650274,  0.01028084,  0.18908545, -0.00852653,  0.04990214,\n",
       "       -0.16647975,  0.03139812, -0.10031002,  0.08946379, -0.09847921,\n",
       "       -0.04651788,  0.00546268, -0.06035695, -0.06309605, -0.05641305,\n",
       "        0.12740244, -0.02256392,  0.0020253 ,  0.02265512,  0.05173049,\n",
       "        0.04746727,  0.03716122,  0.29919875,  0.16928664,  0.00595477,\n",
       "        0.10677752,  0.07032375,  0.11482392, -0.04068539,  0.03147308,\n",
       "       -0.07694066,  0.17424831,  0.00802459, -0.16361476, -0.0118871 ,\n",
       "        0.1564143 ,  0.05716102, -0.04575812,  0.03047533, -0.01532395,\n",
       "       -0.06772293, -0.13137843, -0.11620968,  0.09834027,  0.04665079,\n",
       "       -0.08332928,  0.09796492, -0.10486431,  0.09119651, -0.02637664,\n",
       "        0.07367474,  0.14528666, -0.20880948, -0.10694977, -0.03375373,\n",
       "        0.07699401,  0.11871337,  0.04285951,  0.16123356,  0.15882167],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vectors'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Series name: vectors\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "1490 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df[\"vectors\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../assets/model_Doc2Vec.joblib']"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(df, \"../assets/BBC_News_Vectorized.joblib\")\n",
    "dump(model, \"../assets/model_Doc2Vec.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute most similiar article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(article):\n",
    "    # Tokenize the article\n",
    "    words = article.split()\n",
    "\n",
    "    # Remove punctuation\n",
    "    punctuation = string.punctuation\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores(article, articles_vec, model):\n",
    "    article = preprocess_article(article)\n",
    "    article_vector = model.infer_vector(article)\n",
    "\n",
    "    vectors = articles_vec['vectors']\n",
    "    i = 0\n",
    "    similarity_score_dict = dict()\n",
    "    for vector in vectors:\n",
    "\n",
    "        # Reshape the vectors to be 2D arrays with a single row\n",
    "        vector = vector.reshape(1, -1)\n",
    "        article_vector = article_vector.reshape(1, -1)\n",
    "\n",
    "        similarity_score = cosine_similarity(vector, article_vector)[0, 0]\n",
    "        \n",
    "        similarity_score_dict[i] = similarity_score\n",
    "        i += 1\n",
    "\n",
    "    return similarity_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_articles(article, category):\n",
    "    articles_vec = load(\"../assets/BBC_News_Vectorized.joblib\")\n",
    "    articles_vec.set_index([\"Unnamed: 0\"],inplace=True)\n",
    "    model = load(\"../assets/model_Doc2Vec.joblib\")\n",
    "    \n",
    "    # Get the initial DataFrame with the articles before processing\n",
    "    articles = pd.read_csv(\"../data/BBC_News_Train.csv\")\n",
    "\n",
    "    # add articles to the vectorized df\n",
    "    articles_vec[\"Article\"] = articles[\"Text\"]\n",
    "    \n",
    "    # Filter articles by the specified category\n",
    "    articles_vec = articles_vec[articles_vec[\"Category\"] == category]\n",
    "\n",
    "    scores = compute_similarity_scores(article, articles_vec, model)\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Get the top three keys (indices)\n",
    "    top_three_keys = list(sorted_scores.keys())[:3]\n",
    "    top_three_keys = articles_vec.index[top_three_keys]\n",
    "\n",
    "    # Extract the corresponding articles\n",
    "    top_three_articles = articles_vec.loc[top_three_keys, [\"Article\", \"Category\"]]  \n",
    "\n",
    "    return top_three_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(article):\n",
    "    svc = load(\"../assets/SVC.joblib\")\n",
    "    category = svc.predict([article])[0]\n",
    "    print(f\"Initial article categorized as {category}.\")\n",
    "\n",
    "    recommendations = most_similar_articles(article, category)\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software watching while you work software that can not only monitor every keystroke and action performed at a pc but also be used as legally binding evidence of wrong-doing has been unveiled.  worries about cyber-crime and sabotage have prompted many employers to consider monitoring employees. the developers behind the system claim it is a break-through in the way data is monitored and stored. but privacy advocates are concerned by the invasive nature of such software.  the system is a joint venture between security firm 3ami and storage specialists bridgehead software. they have joined forces to create a system which can monitor computer activity  store it and retrieve disputed files within minutes. more and more firms are finding themselves in deep water as a result of data misuse. sabotage and data theft are most commonly committed from within an organisation according to the national hi-tech crime unit (nhtcu) a survey conducted on its behalf by nop found evidence that more than 80% of medium and large companies have been victims of some form of cyber-crime. bridgehead software has come up with techniques to prove  to a legal standard  that any stored file on a pc has not been tampered with. ironically the impetus for developing the system came as a result of the freedom of information act  which requires companies to store all data for a certain amount of time.  the storage system has been incorporated into an application developed by security firm 3ami which allows every action on a computer to be logged. potentially it could help employers to follow the trail of stolen files and pinpoint whether they had been emailed to a third party  copied  printed  deleted or saved to cd  floppy disk  memory stick or flash card. other activities the system can monitor include the downloading of pornography  the use of racist or bullying language or the copying of applications for personal use. increasingly organisations that handle sensitive data  such as governments  are using biometric log-ins such as fingerprinting to provide conclusive proof of who was using a particular machine at any given time. privacy advocates are concerned that monitoring at work is not only damaging to employee s privacy but also to the relationship between employers and their staff.  that is not the case   said tim ellsmore  managing director of 3ami.  it is not about replacing dialogue but there are issues that you can talk through but you still need proof   he said.  people need to recognise that you are using a pc as a representative of a company and that employers have a legal requirement to store data   he added.\n"
     ]
    }
   ],
   "source": [
    "articles_test = pd.read_csv(\"../data/BBC_News_Test.csv\")\n",
    "article = articles_test.iloc[1][\"Text\"]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial article categroized as tech.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>games maker fights for survival one of britain...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>web radio takes spanish rap global spin the ra...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>cyber criminals step up the pace so-called phi...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Article Category\n",
       "Unnamed: 0                                                            \n",
       "1072        games maker fights for survival one of britain...     tech\n",
       "616         web radio takes spanish rap global spin the ra...     tech\n",
       "899         cyber criminals step up the pace so-called phi...     tech"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBC_news_classification-cIhFtWaR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
