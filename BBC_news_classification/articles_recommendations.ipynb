{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "# import ast\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommending Books using Doc2Vec\n",
    "\n",
    "### Approach:\n",
    "1. **Train a Doc2Vec Model:**\n",
    "   - Utilize Doc2Vec to transform entire articles into continuous vector representations.\n",
    "   - Train the model to capture contextual information and semantic meaning of each document.\n",
    "\n",
    "2. **Compute Similarity:**\n",
    "   - Measure similarity between articles based on the cosine similarity of their Doc2Vec vectors.\n",
    "   - Cosine similarity provides a metric for how closely the semantic meanings align.\n",
    "\n",
    "3. **Benefits of Doc2Vec:**\n",
    "   - Doc2Vec models excel in capturing contextual nuances and semantic relationships within entire documents.\n",
    "\n",
    "This approach enables the recommendation of books based on their semantic similarity, allowing for a more nuanced understanding of content beyond simple keyword matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation of articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/BBC_News_Train_PREPROCESSED.csv\")\n",
    "documents = df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04412849,  0.04196702, -0.1751237 , -0.05107028, -0.02442005,\n",
       "       -0.03083043,  0.03644574, -0.03671683, -0.356493  , -0.05342098,\n",
       "       -0.01799106,  0.07412537,  0.0735252 , -0.00090576,  0.06554895,\n",
       "       -0.1527184 ,  0.17697322,  0.02930997, -0.11212368, -0.23931308,\n",
       "       -0.10960032, -0.1447083 , -0.01049796, -0.06360933, -0.07149747,\n",
       "       -0.12323113, -0.25467908, -0.16970187,  0.08936521, -0.02387957,\n",
       "        0.23850912,  0.08414102, -0.03859787,  0.10782184,  0.03359393,\n",
       "        0.20117247, -0.12455205, -0.0595403 , -0.00265377, -0.02435957,\n",
       "        0.07036934, -0.07602061,  0.04109928,  0.03612635,  0.01235057,\n",
       "        0.00465285,  0.02722651, -0.0761276 ,  0.02662544, -0.13655598,\n",
       "       -0.11038294, -0.11001816, -0.07485324, -0.02365346, -0.11764125,\n",
       "        0.06043214, -0.02589022, -0.00209121,  0.10730504, -0.1039249 ,\n",
       "       -0.00792989, -0.08163863,  0.13263145, -0.06044759, -0.07666414,\n",
       "        0.09355614, -0.02641441,  0.05969002, -0.17673804, -0.08144649,\n",
       "       -0.14490476,  0.08995875, -0.05065875,  0.07998927, -0.00089634,\n",
       "        0.12058666, -0.09849074,  0.03660429,  0.00598852,  0.01684406,\n",
       "       -0.01408631,  0.0619986 , -0.05359407,  0.11456061,  0.14445244,\n",
       "       -0.10366841, -0.11415274,  0.01066168,  0.07864432, -0.04743781,\n",
       "        0.05609181,  0.09095216, -0.23916022, -0.01577189, -0.022112  ,\n",
       "        0.04478642,  0.09041247, -0.01551759,  0.07677438, -0.00781439],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Training the Doc2Vec model\n",
    "vector_size = 100\n",
    "model = Doc2Vec(vector_size=vector_size, window=5, min_count=1, workers=4, epochs=10)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Retrieving vectors for existing documents\n",
    "existing_document_index = 0\n",
    "existing_document_vector = model.dv[existing_document_index]\n",
    "existing_document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vectoriser un nouveau document, il suffira d'Ã©xecuter les lignes suivantes : \n",
    "\n",
    "`new_document = ['new', 'document', 'to', 'vectorize']`<br>\n",
    "`vector = model.infer_vector(new_document)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(row):\n",
    "    return model.dv[row.name]\n",
    "\n",
    "# Apply the function to create the 'vectors' column\n",
    "df['vectors'] = df.apply(get_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04412849,  0.04196702, -0.1751237 , -0.05107028, -0.02442005,\n",
       "       -0.03083043,  0.03644574, -0.03671683, -0.356493  , -0.05342098,\n",
       "       -0.01799106,  0.07412537,  0.0735252 , -0.00090576,  0.06554895,\n",
       "       -0.1527184 ,  0.17697322,  0.02930997, -0.11212368, -0.23931308,\n",
       "       -0.10960032, -0.1447083 , -0.01049796, -0.06360933, -0.07149747,\n",
       "       -0.12323113, -0.25467908, -0.16970187,  0.08936521, -0.02387957,\n",
       "        0.23850912,  0.08414102, -0.03859787,  0.10782184,  0.03359393,\n",
       "        0.20117247, -0.12455205, -0.0595403 , -0.00265377, -0.02435957,\n",
       "        0.07036934, -0.07602061,  0.04109928,  0.03612635,  0.01235057,\n",
       "        0.00465285,  0.02722651, -0.0761276 ,  0.02662544, -0.13655598,\n",
       "       -0.11038294, -0.11001816, -0.07485324, -0.02365346, -0.11764125,\n",
       "        0.06043214, -0.02589022, -0.00209121,  0.10730504, -0.1039249 ,\n",
       "       -0.00792989, -0.08163863,  0.13263145, -0.06044759, -0.07666414,\n",
       "        0.09355614, -0.02641441,  0.05969002, -0.17673804, -0.08144649,\n",
       "       -0.14490476,  0.08995875, -0.05065875,  0.07998927, -0.00089634,\n",
       "        0.12058666, -0.09849074,  0.03660429,  0.00598852,  0.01684406,\n",
       "       -0.01408631,  0.0619986 , -0.05359407,  0.11456061,  0.14445244,\n",
       "       -0.10366841, -0.11415274,  0.01066168,  0.07864432, -0.04743781,\n",
       "        0.05609181,  0.09095216, -0.23916022, -0.01577189, -0.022112  ,\n",
       "        0.04478642,  0.09041247, -0.01551759,  0.07677438, -0.00781439],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vectors'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Series name: vectors\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "1490 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df[\"vectors\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../assets/model_Doc2Vec.joblib']"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(df, \"../assets/BBC_News_Vectorized.joblib\")\n",
    "dump(model, \"../assets/model_Doc2Vec.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute most similiar article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(article):\n",
    "    # Tokenize the article\n",
    "    words = article.split()\n",
    "\n",
    "    # Remove punctuation\n",
    "    punctuation = string.punctuation\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores(article, articles_vec, model):\n",
    "    article = preprocess_article(article)\n",
    "    article_vector = model.infer_vector(article)\n",
    "\n",
    "    vectors = articles_vec['vectors']\n",
    "    i = 0\n",
    "    similarity_score_dict = dict()\n",
    "    for vector in vectors:\n",
    "\n",
    "        # Reshape the vectors to be 2D arrays with a single row\n",
    "        vector = vector.reshape(1, -1)\n",
    "        article_vector = article_vector.reshape(1, -1)\n",
    "\n",
    "        similarity_score = cosine_similarity(vector, article_vector)[0, 0]\n",
    "        \n",
    "        similarity_score_dict[i] = similarity_score\n",
    "        i += 1\n",
    "\n",
    "    return similarity_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_articles(article, category):\n",
    "    # Load precomputed vectors\n",
    "    articles_vec = load(\"../assets/BBC_News_Vectorized.joblib\")\n",
    "    articles_vec.set_index([\"Unnamed: 0\"],inplace=True)\n",
    "    \n",
    "    # Get the initial DataFrame with the articles before processing\n",
    "    articles = pd.read_csv(\"../data/BBC_News_Train.csv\")\n",
    "\n",
    "    # Combine vectors with the original DataFrame\n",
    "    articles_vec[\"Article\"] = articles[\"Text\"]\n",
    "\n",
    "    print(articles_vec.info())\n",
    "    \n",
    "    # Filter articles by the specified category\n",
    "    articles_vec = articles_vec[articles_vec[\"Category\"] == category]\n",
    "\n",
    "    # Load the Doc2Vec model\n",
    "    model = load(\"../assets/model_Doc2Vec.joblib\")\n",
    "\n",
    "    # Compute similarity scores\n",
    "    scores = compute_similarity_scores(article, articles_vec, model)\n",
    "    \n",
    "    # Sort the scores in descending order\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Get the top three keys (indices)\n",
    "    top_three_keys = list(sorted_scores.keys())[:3]\n",
    "    top_three_keys = articles_vec.index[top_three_keys]\n",
    "\n",
    "    # Extract the corresponding articles\n",
    "    top_three_articles = articles_vec.loc[top_three_keys, [\"Article\", \"Category\"]]  \n",
    "\n",
    "    return top_three_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = load(\"../assets/SVC.joblib\")\n",
    "articles_test = pd.read_csv(\"../data/BBC_News_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software watching while you work software that can not only monitor every keystroke and action performed at a pc but also be used as legally binding evidence of wrong-doing has been unveiled.  worries about cyber-crime and sabotage have prompted many employers to consider monitoring employees. the developers behind the system claim it is a break-through in the way data is monitored and stored. but privacy advocates are concerned by the invasive nature of such software.  the system is a joint venture between security firm 3ami and storage specialists bridgehead software. they have joined forces to create a system which can monitor computer activity  store it and retrieve disputed files within minutes. more and more firms are finding themselves in deep water as a result of data misuse. sabotage and data theft are most commonly committed from within an organisation according to the national hi-tech crime unit (nhtcu) a survey conducted on its behalf by nop found evidence that more than 80% of medium and large companies have been victims of some form of cyber-crime. bridgehead software has come up with techniques to prove  to a legal standard  that any stored file on a pc has not been tampered with. ironically the impetus for developing the system came as a result of the freedom of information act  which requires companies to store all data for a certain amount of time.  the storage system has been incorporated into an application developed by security firm 3ami which allows every action on a computer to be logged. potentially it could help employers to follow the trail of stolen files and pinpoint whether they had been emailed to a third party  copied  printed  deleted or saved to cd  floppy disk  memory stick or flash card. other activities the system can monitor include the downloading of pornography  the use of racist or bullying language or the copying of applications for personal use. increasingly organisations that handle sensitive data  such as governments  are using biometric log-ins such as fingerprinting to provide conclusive proof of who was using a particular machine at any given time. privacy advocates are concerned that monitoring at work is not only damaging to employee s privacy but also to the relationship between employers and their staff.  that is not the case   said tim ellsmore  managing director of 3ami.  it is not about replacing dialogue but there are issues that you can talk through but you still need proof   he said.  people need to recognise that you are using a pc as a representative of a company and that employers have a legal requirement to store data   he added.\n"
     ]
    }
   ],
   "source": [
    "article = articles_test.iloc[1][\"Text\"]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = svc.predict([article])[0]\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems coherent and correct. Let us continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1490 entries, 0 to 1489\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      " 3   vectors    1490 non-null   object\n",
      " 4   Article    1490 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 69.8+ KB\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([22, 252, 21], dtype='int32', name='Unnamed: 0')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[319], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmost_similar_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[315], line 30\u001b[0m, in \u001b[0;36mmost_similar_articles\u001b[1;34m(article, category)\u001b[0m\n\u001b[0;32m     27\u001b[0m top_three_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sorted_scores\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Extract the corresponding articles\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m top_three_articles \u001b[38;5;241m=\u001b[39m \u001b[43marticles_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_three_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m top_three_articles\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexing.py:1337\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m-> 1337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexing.py:1288\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[1;32m-> 1288\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1289\u001b[0m     axis: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1291\u001b[0m }\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexing.py:1289\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1289\u001b[0m     axis: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1291\u001b[0m }\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haddo\\.virtualenvs\\BBC_news_classification-cIhFtWaR\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([22, 252, 21], dtype='int32', name='Unnamed: 0')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "most_similar_articles(article, category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBC_news_classification-cIhFtWaR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
